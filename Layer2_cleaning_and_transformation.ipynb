{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b403e3a-19bf-4461-a267-69647657acf1",
   "metadata": {},
   "source": [
    "Data Cleaning & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f72c3df9-f4b0-4a89-b6d0-64db49d96049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load the dataset and identify date columns\n",
    "df = pd.read_csv('layer2.csv')\n",
    "date_cols = [col for col in df.columns if col not in ['id', 'symbol', 'name', 'platforms']]\n",
    "\n",
    "# Melt wide format to long format (Token, Date, Price)\n",
    "df_long = df.melt(id_vars=['symbol', 'name', 'platforms'], value_vars=date_cols, \n",
    "                  var_name='Date', value_name='Price')\n",
    "df_long.dropna(subset=['Price'], inplace=True)            # remove rows with no price data\n",
    "df_long['Date'] = pd.to_datetime(df_long['Date'], format='%m/%d/%y')  # parse dates\n",
    "\n",
    "# Clean Platform field (remove newlines for CSV cleanliness)\n",
    "df_long['platforms'] = df_long['platforms'].str.replace(r'\\n\\s*', ' ', regex=True)\n",
    "\n",
    "# Rename and organize columns\n",
    "df_long.drop(columns=['symbol'], inplace=True)            # drop token symbol (not needed in output)\n",
    "df_long.rename(columns={'name': 'Token', 'platforms': 'Platform'}, inplace=True)\n",
    "\n",
    "# Sort by Token and Date (ensuring each token's data is chronological)\n",
    "# Preserve original token order (as in input file) when sorting\n",
    "token_order = {name: i for i, name in enumerate(df['name'])}\n",
    "df_long['TokenOrder'] = df_long['Token'].map(token_order)\n",
    "df_long.sort_values(['TokenOrder', 'Date'], inplace=True)\n",
    "df_long.drop(columns=['TokenOrder'], inplace=True)        # drop helper column\n",
    "\n",
    "# 2. Calculate daily percentage returns per token\n",
    "df_long['Daily Return'] = df_long.groupby('Token')['Price'].pct_change()\n",
    "\n",
    "# 3. Create Indexed Price (base = 100 at first date for each token)\n",
    "df_long['Indexed Price'] = df_long['Price'] / df_long.groupby('Token')['Price'].transform('first') * 100\n",
    "\n",
    "# 4. Compute per-token statistics: Avg Return, Volatility, Total Return\n",
    "stats = df_long.groupby('Token').agg(\n",
    "    **{\n",
    "        'Avg Return': ('Daily Return', 'mean'), \n",
    "        'Volatility': ('Daily Return', 'std'),\n",
    "        'Total Return': ('Price', lambda x: x.iloc[-1] / x.iloc[0] - 1)\n",
    "    }\n",
    ")\n",
    "stats.fillna(0.0, inplace=True)  # if a token has no variation (e.g., only one price), fill stats with 0\n",
    "stats.reset_index(inplace=True)\n",
    "\n",
    "# 5. Merge statistics back into the long dataframe\n",
    "df_final = pd.merge(df_long, stats, on='Token', how='left')\n",
    "\n",
    "# Add Volume column (not available in source, so fill with NaN/empty)\n",
    "df_final['Volume'] = np.nan\n",
    "\n",
    "# Reorder columns to the desired output order\n",
    "df_final = df_final[[\n",
    "    'Date', 'Token', 'Price', 'Volume', 'Daily Return', \n",
    "    'Indexed Price', 'Avg Return', 'Volatility', 'Total Return', 'Platform'\n",
    "]]\n",
    "\n",
    "# 6. Output to CSV\n",
    "df_final.to_csv('layer2_tableau_ready.csv', index=False, date_format='%Y-%m-%d')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1080cec5-c85a-4635-837d-22d27252de19",
   "metadata": {},
   "source": [
    "Generating Token Correlation Data for Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23b53cd-394f-4a67-bf8e-82adefddb797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix saved to 'layer2_correlations.csv'.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the processed Layer 2 dataset (the one with daily returns, created previously)\n",
    "df_final = pd.read_csv('layer2_tableau_ready.csv')\n",
    "\n",
    "# Expected columns in df_final include:\n",
    "#   Date, Token, Price, Volume, Daily Return, Indexed Price, \n",
    "#   Avg Return, Volatility, Total Return, Platform\n",
    "\n",
    "# 2. Pivot the table so each Token's daily returns become a separate column\n",
    "#    Rows = distinct dates, Columns = token names, Values = daily returns\n",
    "df_pivot = df_final.pivot_table(\n",
    "    index='Date',         # each row = one date\n",
    "    columns='Token',      # each column = one token\n",
    "    values='Daily Return' # the numeric values = daily returns\n",
    ")\n",
    "\n",
    "# 3. Compute the correlation matrix\n",
    "#    This will produce an N x N matrix, where N is the number of tokens\n",
    "corr_matrix = df_pivot.corr()  # default = Pearson correlation of columns\n",
    "\n",
    "# 4. Convert the wide correlation matrix into a long format:\n",
    "#    Columns: ['Token', 'Token2', 'Correlation']\n",
    "corr_long = corr_matrix.reset_index().melt(\n",
    "    id_vars='Token',\n",
    "    var_name='Token2',\n",
    "    value_name='Correlation'\n",
    ")\n",
    "\n",
    "# 5. Save the correlation pairs to a new CSV\n",
    "corr_long.to_csv('layer2_correlations.csv', index=False)\n",
    "\n",
    "print(\"Correlation matrix saved to 'layer2_correlations.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d800928-680d-43be-a034-05f8c7d9cfe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
